
- 在 Update 一行数据的时候，会上什么锁

  总体而言，这需要区分事务隔离级别，以及 Update 语句中的条件
  
  首先， MySQL的锁，锁的范围上有分表锁，行锁，全局锁， 锁的可见性上有排它锁（X锁），共享锁（S锁）
  
  其次，从事务隔离级别来说，RU(READ UNCOMMITTED) 不会加锁所以不考虑，Serializable 事务串行化执行也不存在多个事务同时抢锁的情况，那么接下来只需要针对 RC 和 RR 两个常用的事务隔离级别考虑。
  
  而 Update 中的语句可以分成主键聚簇索引，非聚簇索引的唯一索引，非唯一索引（包括单个和范围）

  - 如果查询条件是主键，那么 RC 和 RR 都会对应这条 id 的行锁
  - 如果查询条件是唯一索引，那么 RC 和 RR 对应对这个唯一索引对应的节点，以及聚簇索引对应的 id 都会加上索航
  - 如果查询条件是非唯一索引，满足查询条件的行以及对应的聚簇索引都会加上锁（当然在 RR 下还会根据加间隙锁防止幻读）
  - 如果查询条件没有特定到一些行，就会全局锁表了

- TCC 方案中的幂等，悬挂和空回滚是什么

  TCC 方案中会存在一个全局事务协调器，以及多个业务服务（ 每个服务都实现了约定好的try/commit/cancel）接口，且这 try 之后才会执行 commit 或者 cancel

  - 幂等指的是因为网络原因或者服务故障等原因，导致重复调用 try/commit/cancel 方法的情况，解决方案是需要所有的分布式事务方法都必须保持幂等。
  - 悬挂指的是二阶段的 commit 或者 cancel 比 try 先执行的情况，解决方案是是使用全局事务 id 去做处理，会对业务服务的负担比较重
  - 空回滚和悬挂的情况有点类似，指的是没有调用 try 方法却调用了 cancel 的情况。解决方案是要支持空回滚的情况，去结束当前的分布式事务

- 基于 redis 实现的分布式锁，怎么避免死锁，锁提前过期以及被别人释放这三种情况

  - 避免死锁 死锁出现的根本原因是加锁之后没有设置到过期时间。原因是 redis 加锁和设置过期时间不是原子操作。有可能加锁之后，redis、加锁方、网络问题等有执行到设置过期时间。不过解决方案是 redis 在接下来的版本提供了一个加锁且可以设置过期时间的原子性命令

  - 锁提前过期 提前过期的原因是，设置过期时间的预估不理想，或者执行的任务时间是在太长了。在 redission 的解决方案中，是设置一个守护线程去对过期时间进行续约，当然也要去查看加锁的线程是否存活被其他线程释放锁

- kafka 如何保证 exactly once

  kafka 的 exactly once 是依靠新版本的**幂等**和**事务**两个特性去保证。
  - 幂等的特性指的是使用消息的序列号去做消息去重和保证消息有序。生产者给每次投递的消息设置一个单调递增的序列号，接收到消息的 broker 如果当前收到的序号不是递增顺序的就会拒绝，来保证中间缺失的序号一定要先到，并且生产者也会不断去做重试，和 TCP 的 seq 去重和重传机制类似。
  - 事务的特性指的是将一堆消息投递操作当做一个原子操作去进行投递，引入事务 ID 去做幂等发送和事务恢复

- kafka 中什么是 HW （高水位）和 LEO （日志末端位移）
  - HW High Watermark 指的是「当前日志文件」一个特定的消息偏移量 offset，标识着消费者只能拉取这个 offset 之前的消息
  - LEO Log End offset 指的是「当前日志文件」下一条待写入消息的 offset

  每个 partition 的 replication 都会维护自身的 LEO，整个 ISR 集合中最小的 LEO 就是全部 replication 的 HW，消费者只能看到 HW 之前的消息

- kafka 为什么是主写主读而不是读写分离

  - 1. 应用场景不合适。 kafka 属于多读多写的中间件，而读写分离更适用于多读少写的场景。

  数据一致性问题。由于主从同步是有延时和短暂的数据不一致的， kafka 中生产和消费消息都需要频繁读写，如果使用了读写分离中的向 follower 读和写在 leader ，会难以保证处理消息时是顺序的（因为主从同步一般会选择主从异步复制，会有可能导致丢消息。如果要全同步的话会降低整个服务的吞吐）

  - 2.kafka 的设计让主写主读的场景更优于读写分离。kafka 的初衷设计是以细粒度的 partition 维度提供对外服务读写服务，足够细的 partition 以及分区规则能让整个服务的吞吐很高，并且能够实现负载均衡

- 
