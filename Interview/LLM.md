- 介绍CUDA的存储结构。 
- 说说CUDA对主存的抽象。 
- 介绍CUDA缓存。 
- 介绍CUDA的线程/线程块/线程网格的概念。 
- 介绍CUDA的wrap。 
- GPU优化卷积有什么策略。 
- GPU优化转置有什么策略。 
- GPU优化矩阵乘有什么方式。 
- 介绍flash attention的优化（1和2都需要）。（要求） 
- 介绍并行排序，能写一下吗。 
- CUDA程序怎么profile。 
- 介绍tensorcore。 
- 说一下你知道的卡的参数和通信参数。（要求） 
- NVLink介绍一下，RDMA介绍一下。 
- 你有什么程序性能优化的经验。 
- 介绍常见模型压缩的方法（量化剪枝蒸馏）。 
- 量化分几种。 你实际用过什么量化。
- 介绍一下tensorrt的部署流程。 
- 介绍一下tensorrt量化和自定义算子的手段。 
- 介绍一下ONNXruntime的系统图。 
- 介绍一下TVM的部署流程以及它会用到什么优化。 
- 介绍TVM搜索超参的行为。 
- 分布式训练包括什么手段。 
- 数据并行和分布式数据并行区别。 背一下相关的通信指标。 
- 模型并行中流水线空泡说一下怎么做。 
- FSDP，zero的三阶段说一下。 
- 集合通信原语说一下，all reduce/gather优化。
- 多节点如何通信优化。
- 思维链和思维树的区别，思维树的每个节点是等可能的吗，会有无限的路径吗，思维树的结果是怎么得出来的。
  - 建议回复：不总是相等，取决于ToT的策略和评估函数所以ToT一般是控制深度，模式上类似于BFS
- Gbdt和xgboost的区别
  - XGBoost是对GBDT的改进和扩展，它提供了更高的效率、更好的性能、正则化技术、内置特征选择等功能。
    - (1)正则化：
      - GBDT使用基本的树模型，并在每一轮迭代中逐渐增加树的复杂性。它使用简单的正则化技术，如叶子节点的最小样本数限制，来防止过拟合。
      - XGBoost引入了正则化技术，包括L1和L2正则化，以减少过拟合风险。它还使用了二阶导数信息来提高训练的稳定性。
    - (2)高效性：
      - XGBoost通过多线程和分布式计算提供了更高的训练效率。它实现了高度优化的数据存储和计算，以减少内存使用和加速训练过程。
      - GBDT通常以串行方式训练，训练时间可能较长，特别是在处理大规模数据时。
    - (3)缺失值处理：
      - XGBoost能够自动处理缺失值，无需手动进行处理。
      - 在GBDT中，需要在数据预处理阶段手动处理缺失值，通常通过填充或删除缺失值。
    - (4)内置特征选择：
      - XGBoost具有内置的特征选择功能，它可以估计每个特征的重要性，并根据其重要性进行特征选择。GBDT通常需要手动进行特征选择或依赖其他特征选择方法。
    - (5)求导优化：
      - GBDT只需要对目标函数求一阶导，xgboost要求二阶导。
- Lstm的特点
  - (1)门控机制：
    - LSTM引入了门控机制，包括遗忘门、输入门和输出门，这些门控制着信息的流动和保存。遗忘门决定哪些信息应该被遗忘，输入门控制哪些信息应该被添加到记忆单元，输出门控制什么信息 应该传递到下一个时间步。这种机制有助于控制信息的流动，提高了模型的训练效率。
  - (2)长期记忆：
    - LSTM的主要特点是能够捕捉和维护长期依赖关系，它在处理序列数据中表现出色。传统的RNN存在梯度消失问题，导致难以学习长序列的依赖关系，而LSTM通过设计具有记忆单元的结构来解  - 决这个问题，允许信息在长时间内保持不变。
  - (3)平行化训练：
    - LSTM具有良好的并行性，可以加速训练过程，特别是在GPU上进行训练。这有助于处理大规模数据和加速深度学习模型的训练。
- Transformer的最重要的特点，对比CNN的效果
  - 最重要的特点是自注意力机制。
   - 对比CNN，transformer更注重全局特征，特征之间能并行计算，CNN更注重局部特征，图像分类领域中，在图像数量充足的情况下，tranformer的效果通常比CNN好
- ReLU激活函数的优缺点，怎么改进
  - 优点：
  - (1) 当特征值大于0时，可以避免梯度消失
  - (2)计算简单
  - 缺点：
  - (1) 非零均值
  - (2)当特征值大量小于0时，可能引起梯度消失
  - (3)当特征值大于0时，非线性拟合能力可能下降
  - 改进：改用Leaky ReLU函数
- 样本不均衡的处理方法
  - (1)欠采样
  - (2)过采样
  - (3)平衡读取数据
  - (4)设置权重，对样本较少的数据设置较高的训练权重
  - (5)使用平衡损失函数，比如focal loss等
  - (6)数据增强
- 介绍Focal loss
  - Focal Loss 最初由物体检测领域的研究者提出，其主要目标是减轻模型在训练过程中对大多数背景类别的关注，从而更好地处理少数类别的样本。这种损失函数有助于提高模型对罕见类别的检测性能。
  - Focal Loss 的主要特点如下:
   - 关注难分样本：Focal Loss 通过调整样本的权重，更加关注难以分类的样本。通常情况下，容易分类的样本(大多数属于背景类别)会降低其权重，而难分类的样本(属于少数类别)会增加其权重。
   - 降低易分类样本的权重：通过调整损失函数，Focal Loss 能够有效地降低容易分类的样本(背景类别、样本数量多的类别)的权重，这样模型将更加关注罕见类别，从而提高了模型在罕见类别的检测能力。
   - Focal Loss 的引入有助于提高目标检测模型对于罕见目标的检测性能，减轻了类别不平衡问题对模型训练的影响。
